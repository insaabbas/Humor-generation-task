{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insaabbas/Humor-generation-task/blob/main/4_epoch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU8SnzYrcvGc"
      },
      "outputs": [],
      "source": [
        "# 1. SETUP & DRIVE MOUNT\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q -U transformers peft bitsandbytes datasets accelerate\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
        "    AutoConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 2. DATA PREP (Required to avoid NameError)\n",
        "model_name = \"microsoft/phi-2\"\n",
        "df = pd.read_csv('jokes.tsv', sep='\\t')\n",
        "def format_instruction(example):\n",
        "    return {\"text\": f\"### Input: {example['input']}\\n### Response: JOKE: {example['joke']}\"}\n",
        "\n",
        "dataset = Dataset.from_pandas(df).map(format_instruction)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=200, padding=\"max_length\")\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "# 3. MODEL LOADING with PHI-2 FIX\n",
        "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, config=config, quantization_config=bnb_config,\n",
        "    trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "# 4. LoRA SETUP (r=16 as per your specific code)\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"Wqkv\", \"fc1\", \"fc2\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# 5. TRAINING ARGUMENTS\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/phi2-joke-trainer\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=4,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "# Fix for Hub Token bug\n",
        "if not hasattr(training_args, 'push_to_hub_token'):\n",
        "    training_args.push_to_hub_token = None\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# ==========================================================\n",
        "# 6. RESUME COMMAND\n",
        "# ==========================================================\n",
        "# This looks for the latest checkpoint in your output_dir\n",
        "trainer.train(resume_from_checkpoint=True)\n",
        "\n",
        "# Save Final\n",
        "trainer.model.save_pretrained(\"/content/drive/MyDrive/phi2-joke-model-final\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/phi2-joke-model-final\")\n",
        "\n",
        "print(\"RESUME COMPLETE! Model saved to 'phi2-joke-model-final'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd6W52GOb_OI"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes peft datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf9C04uT7M5V"
      },
      "source": [
        "MERGING AND PUSHING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtVducxFbOjM"
      },
      "outputs": [],
      "source": [
        "# 1. FORCE MOUNT DRIVE\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 2. VERIFY THE FOLDER EXISTS\n",
        "target_folder = \"phi2-joke-model-final\"\n",
        "base_path = \"/content/drive/MyDrive/\"\n",
        "full_path = os.path.join(base_path, target_folder)\n",
        "\n",
        "if not os.path.exists(full_path):\n",
        "    print(f\"❌ Still can't find {target_folder}!\")\n",
        "    print(\"Listing what IS in your Drive:\")\n",
        "    print(os.listdir(base_path))\n",
        "else:\n",
        "    print(f\"✅ Found it! Path is: {full_path}\")\n",
        "\n",
        "    # 3. RUN THE MERGE\n",
        "    import torch\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "    from peft import PeftModel\n",
        "    from huggingface_hub import login\n",
        "\n",
        "    # LOGIN (Use 'Write' token)\n",
        "    login()\n",
        "\n",
        "    hf_username = \"insaabbas\"\n",
        "    new_model_name = f\"{hf_username}/phi2-4-epoch-humor-model\"\n",
        "    base_model_name = \"microsoft/phi-2\"\n",
        "\n",
        "    print(\"Loading base model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    config = AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "    config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        config=config,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"cpu\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print(\"Merging adapters...\")\n",
        "    model = PeftModel.from_pretrained(base_model, full_path)\n",
        "    merged_model = model.merge_and_unload()\n",
        "\n",
        "    print(f\"Pushing to Hub: {new_model_name}\")\n",
        "    merged_model.push_to_hub(new_model_name)\n",
        "    tokenizer.push_to_hub(new_model_name)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(f\"VICTORY! 4-Epoch model live at: https://huggingface.co/{new_model_name}\")\n",
        "    print(\"=\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8uo1ln-7G5u"
      },
      "source": [
        "OUTPUT FILE GENERATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hSZfMF7i7FR_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, CodeGenTokenizerFast, pipeline\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. SETTINGS\n",
        "MODEL_ID = \"insaabbas/phi2-4-epoch-humor-model\"\n",
        "INPUT_TSV = \"/content/task-a-en.tsv\"\n",
        "OUTPUT_TSV = \"/content/predictions_4epoch.tsv\"\n",
        "\n",
        "# 2. LOAD MODEL & TOKENIZER (With the Fast Tokenizer fix)\n",
        "print(f\"Loading merged 4-epoch model: {MODEL_ID}\")\n",
        "\n",
        "# Forcing CodeGenTokenizerFast to avoid the 'TokenizersBackend' ValueError\n",
        "tokenizer = CodeGenTokenizerFast.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Initialize the generation pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# 3. DATA PROCESSING\n",
        "if not os.path.exists(INPUT_TSV):\n",
        "    print(f\"Error: {INPUT_TSV} not found. Please upload it to Colab.\")\n",
        "else:\n",
        "    # Load your input TSV\n",
        "    df = pd.read_csv(INPUT_TSV, sep='\\t', keep_default_na=False)\n",
        "    results = []\n",
        "\n",
        "    print(f\"Generating jokes for {len(df)} rows using 4-epoch model...\")\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        # Extract prompt context\n",
        "        headline = str(row.get('headline', '')).strip()\n",
        "        word1 = str(row.get('word1', '')).strip()\n",
        "        word2 = str(row.get('word2', '')).strip()\n",
        "\n",
        "        # Match your training input format\n",
        "        input_text = headline if (headline and headline != '-') else f\"{word1} {word2}\"\n",
        "        prompt = f\"### Input: {input_text}\\n### Response: JOKE: \"\n",
        "\n",
        "        # Generate the joke\n",
        "        output = generator(\n",
        "            prompt,\n",
        "            max_new_tokens=64,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            return_full_text=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Clean up the output to keep it on one line\n",
        "        generated_text = output[0]['generated_text'].strip().split('\\n')[0]\n",
        "\n",
        "        # Ensure the mandatory JOKE: prefix\n",
        "        clean_joke = generated_text if generated_text.startswith(\"JOKE:\") else f\"JOKE: {generated_text}\"\n",
        "\n",
        "        results.append({\n",
        "            \"id\": row['id'],\n",
        "            \"joke\": clean_joke\n",
        "        })\n",
        "\n",
        "    # 4. SAVE FINAL TSV\n",
        "    output_df = pd.DataFrame(results)\n",
        "    output_df.to_csv(OUTPUT_TSV, sep='\\t', index=False)\n",
        "\n",
        "    print(f\"\\n--- SUCCESS ---\")\n",
        "    print(f\"4-Epoch predictions saved to: {OUTPUT_TSV}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}